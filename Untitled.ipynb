{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkVI9tdKxZ5i4/9SFdW81A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielNorth/NLU-2023-Labs/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn_crfsuite\n",
        "!pip install es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qb5Mt1VvhILf",
        "outputId": "1a8ecbf8-8dcf-4e11-e52a-4604303a3918"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.65.0)\n",
            "Collecting es_core_news_sm\n",
            "  Downloading es_core_news_sm-3.1.0-py3-none-any.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy<3.2.0,>=3.1.0 (from es_core_news_sm)\n",
            "  Downloading spacy-3.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (3.0.8)\n",
            "Collecting thinc<8.1.0,>=8.0.12 (from spacy<3.2.0,>=3.1.0->es_core_news_sm)\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (0.7.9)\n",
            "Collecting wasabi<1.1.0,>=0.8.1 (from spacy<3.2.0,>=3.1.0->es_core_news_sm)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.0.8)\n",
            "Collecting typer<0.5.0,>=0.3.0 (from spacy<3.2.0,>=3.1.0->es_core_news_sm)\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.27.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.2.0,>=3.1.0->es_core_news_sm)\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.2.0,>=3.1.0->es_core_news_sm) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->es_core_news_sm) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es_core_news_sm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es_core_news_sm) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es_core_news_sm) (3.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->es_core_news_sm) (8.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->es_core_news_sm) (2.1.3)\n",
            "Installing collected packages: wasabi, typer, pydantic, thinc, spacy, es_core_news_sm\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.11\n",
            "    Uninstalling pydantic-1.10.11:\n",
            "      Successfully uninstalled pydantic-1.10.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.10\n",
            "    Uninstalling thinc-8.1.10:\n",
            "      Successfully uninstalled thinc-8.1.10\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.4\n",
            "    Uninstalling spacy-3.5.4:\n",
            "      Successfully uninstalled spacy-3.5.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.1.7 which is incompatible.\n",
            "inflect 6.0.5 requires pydantic<2,>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed es_core_news_sm-3.1.0 pydantic-1.8.2 spacy-3.1.7 thinc-8.0.17 typer-0.4.2 wasabi-0.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydantic",
                  "spacy",
                  "thinc",
                  "wasabi"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AgbjyTpg884",
        "outputId": "277396b8-f1ce-46ff-930e-45216111c9bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:887: UserWarning: [W095] Model 'es_core_news_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.5.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  ) from None\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import conll2002\n",
        "from sklearn_crfsuite import CRF\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import es_core_news_sm\n",
        "\n",
        "import re\n",
        "\n",
        "# !pip install sklearn_crfsuite\n",
        "# !pip install es_core_news_sm\n",
        "\n",
        "\n",
        "def stats():\n",
        "    return {'cor': 0, 'hyp': 0, 'ref': 0}\n",
        "\n",
        "\n",
        "def evaluate(ref, hyp, otag='O'):\n",
        "    # evaluation for NLTK\n",
        "    aligned = align_hyp(ref, hyp)\n",
        "    return conlleval(aligned, otag=otag)\n",
        "\n",
        "\n",
        "def align_hyp(ref, hyp):\n",
        "    # align references and hypotheses for evaluation\n",
        "    # add last element of token tuple in hyp to ref\n",
        "    if len(ref) != len(hyp):\n",
        "        raise ValueError(\"Size Mismatch: ref: {} & hyp: {}\".format(len(ref), len(hyp)))\n",
        "\n",
        "    out = []\n",
        "    for i in range(len(ref)):\n",
        "        if len(ref[i]) != len(hyp[i]):\n",
        "            raise ValueError(\"Size Mismatch: ref: {} & hyp: {}\".format(len(ref), len(hyp)))\n",
        "        out.append([(*ref[i][j], hyp[i][j][-1]) for j in range(len(ref[i]))])\n",
        "    return out\n",
        "\n",
        "\n",
        "def conlleval(data, otag='O'):\n",
        "    # token, segment & class level counts for TP, TP+FP, TP+FN\n",
        "    tok = stats()\n",
        "    seg = stats()\n",
        "    cls = {}\n",
        "\n",
        "    for sent in data:\n",
        "\n",
        "        prev_ref = otag      # previous reference label\n",
        "        prev_hyp = otag      # previous hypothesis label\n",
        "        prev_ref_iob = None  # previous reference label IOB\n",
        "        prev_hyp_iob = None  # previous hypothesis label IOB\n",
        "\n",
        "        in_correct = False  # currently processed chunks is correct until now\n",
        "\n",
        "        for token in sent:\n",
        "\n",
        "            hyp_iob, hyp = parse_iob(token[-1])\n",
        "            ref_iob, ref = parse_iob(token[-2])\n",
        "\n",
        "            ref_e = is_eoc(ref, ref_iob, prev_ref, prev_ref_iob, otag)\n",
        "            hyp_e = is_eoc(hyp, hyp_iob, prev_hyp, prev_hyp_iob, otag)\n",
        "\n",
        "            ref_b = is_boc(ref, ref_iob, prev_ref, prev_ref_iob, otag)\n",
        "            hyp_b = is_boc(hyp, hyp_iob, prev_hyp, prev_hyp_iob, otag)\n",
        "\n",
        "            if not cls.get(ref) and ref:\n",
        "                cls[ref] = stats()\n",
        "\n",
        "            if not cls.get(hyp) and hyp:\n",
        "                cls[hyp] = stats()\n",
        "\n",
        "            # segment-level counts\n",
        "            if in_correct:\n",
        "                if ref_e and hyp_e and prev_hyp == prev_ref:\n",
        "                    in_correct = False\n",
        "                    seg['cor'] += 1\n",
        "                    cls[prev_ref]['cor'] += 1\n",
        "\n",
        "                elif ref_e != hyp_e or hyp != ref:\n",
        "                    in_correct = False\n",
        "\n",
        "            if ref_b and hyp_b and hyp == ref:\n",
        "                in_correct = True\n",
        "\n",
        "            if ref_b:\n",
        "                seg['ref'] += 1\n",
        "                cls[ref]['ref'] += 1\n",
        "\n",
        "            if hyp_b:\n",
        "                seg['hyp'] += 1\n",
        "                cls[hyp]['hyp'] += 1\n",
        "\n",
        "            # token-level counts\n",
        "            if ref == hyp and ref_iob == hyp_iob:\n",
        "                tok['cor'] += 1\n",
        "\n",
        "            tok['ref'] += 1\n",
        "\n",
        "            prev_ref = ref\n",
        "            prev_hyp = hyp\n",
        "            prev_ref_iob = ref_iob\n",
        "            prev_hyp_iob = hyp_iob\n",
        "\n",
        "        if in_correct:\n",
        "            seg['cor'] += 1\n",
        "            cls[prev_ref]['cor'] += 1\n",
        "\n",
        "    return summarize(seg, cls)\n",
        "\n",
        "\n",
        "def parse_iob(t):\n",
        "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
        "    return m.groups() if m else (t, None)\n",
        "\n",
        "\n",
        "def is_boc(lbl, iob, prev_lbl, prev_iob, otag='O'):\n",
        "    \"\"\"\n",
        "    is beginning of a chunk\n",
        "\n",
        "    supports: IOB, IOBE, BILOU schemes\n",
        "        - {E,L} --> last\n",
        "        - {S,U} --> unit\n",
        "\n",
        "    :param lbl: current label\n",
        "    :param iob: current iob\n",
        "    :param prev_lbl: previous label\n",
        "    :param prev_iob: previous iob\n",
        "    :param otag: out-of-chunk label\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    boc = False\n",
        "\n",
        "    boc = True if iob in ['B', 'S', 'U'] else boc\n",
        "    boc = True if iob in ['E', 'L'] and prev_iob in ['E', 'L', 'S', otag] else boc\n",
        "    boc = True if iob == 'I' and prev_iob in ['S', 'L', 'E', otag] else boc\n",
        "\n",
        "    boc = True if lbl != prev_lbl and iob != otag and iob != '.' else boc\n",
        "\n",
        "    # these chunks are assumed to have length 1\n",
        "    boc = True if iob in ['[', ']'] else boc\n",
        "\n",
        "    return boc\n",
        "\n",
        "\n",
        "def is_eoc(lbl, iob, prev_lbl, prev_iob, otag='O'):\n",
        "    \"\"\"\n",
        "    is end of a chunk\n",
        "\n",
        "    supports: IOB, IOBE, BILOU schemes\n",
        "        - {E,L} --> last\n",
        "        - {S,U} --> unit\n",
        "\n",
        "    :param lbl: current label\n",
        "    :param iob: current iob\n",
        "    :param prev_lbl: previous label\n",
        "    :param prev_iob: previous iob\n",
        "    :param otag: out-of-chunk label\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    eoc = False\n",
        "\n",
        "    eoc = True if iob in ['E', 'L', 'S', 'U'] else eoc\n",
        "    eoc = True if iob == 'B' and prev_iob in ['B', 'I'] else eoc\n",
        "    eoc = True if iob in ['S', 'U'] and prev_iob in ['B', 'I'] else eoc\n",
        "\n",
        "    eoc = True if iob == otag and prev_iob in ['B', 'I'] else eoc\n",
        "\n",
        "    eoc = True if lbl != prev_lbl and iob != otag and prev_iob != '.' else eoc\n",
        "\n",
        "    # these chunks are assumed to have length 1\n",
        "    eoc = True if iob in ['[', ']'] else eoc\n",
        "\n",
        "    return eoc\n",
        "\n",
        "\n",
        "def score(cor_cnt, hyp_cnt, ref_cnt):\n",
        "    # precision\n",
        "    p = 1 if hyp_cnt == 0 else cor_cnt / hyp_cnt\n",
        "    # recall\n",
        "    r = 0 if ref_cnt == 0 else cor_cnt / ref_cnt\n",
        "    # f-measure (f1)\n",
        "    f = 0 if p+r == 0 else (2*p*r)/(p+r)\n",
        "    return {\"p\": p, \"r\": r, \"f\": f, \"s\": ref_cnt}\n",
        "\n",
        "\n",
        "def summarize(seg, cls):\n",
        "    # class-level\n",
        "    res = {lbl: score(cls[lbl]['cor'], cls[lbl]['hyp'], cls[lbl]['ref']) for lbl in set(cls.keys())}\n",
        "    # micro\n",
        "    res.update({\"total\": score(seg.get('cor', 0), seg.get('hyp', 0), seg.get('ref', 0))})\n",
        "    return res\n",
        "\n",
        "\n",
        "def read_corpus_conll(corpus_file, fs=\"\\t\"):\n",
        "    \"\"\"\n",
        "    read corpus in CoNLL format\n",
        "    :param corpus_file: corpus in conll format\n",
        "    :param fs: field separator\n",
        "    :return: corpus\n",
        "    \"\"\"\n",
        "    featn = None  # number of features for consistency check\n",
        "    sents = []  # list to hold words list sequences\n",
        "    words = []  # list to hold feature tuples\n",
        "\n",
        "    for line in open(corpus_file):\n",
        "        line = line.strip()\n",
        "        if len(line.strip()) > 0:\n",
        "            feats = tuple(line.strip().split(fs))\n",
        "            if not featn:\n",
        "                featn = len(feats)\n",
        "            elif featn != len(feats) and len(feats) != 0:\n",
        "                raise ValueError(\"Unexpected number of columns {} ({})\".format(len(feats), featn))\n",
        "\n",
        "            words.append(feats)\n",
        "        else:\n",
        "            if len(words) > 0:\n",
        "                sents.append(words)\n",
        "                words = []\n",
        "    return sents\n",
        "\n",
        "\n",
        "def get_chunks(corpus_file, fs=\"\\t\", otag=\"O\"):\n",
        "    sents = read_corpus_conll(corpus_file, fs=fs)\n",
        "    return set([parse_iob(token[-1])[1] for sent in sents for token in sent if token[-1] != otag])\n",
        "\n",
        "\n",
        "# Function to extract features using sent2spacy_features\n",
        "def sent2spacy_features(sent):\n",
        "    nlp = es_core_news_sm.load()\n",
        "    nlp.tokenizer = Tokenizer(nlp.vocab)\n",
        "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
        "    feats = []\n",
        "    for token in spacy_sent:\n",
        "        token_feats = {\n",
        "            'bias': 1.0,\n",
        "            'word.lower()': token.lower_,\n",
        "            'pos': token.pos_,\n",
        "            'lemma': token.lemma_\n",
        "        }\n",
        "        feats.append(token_feats)\n",
        "    return feats\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "# Function to extract tokens using sent2tokens\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, pos, iob in sent]\n",
        "\n",
        "# Function to extract features for the tutorial\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    pos = sent[i][1]\n",
        "    return {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'pos': pos,\n",
        "        'pos[:2]': pos[:2]\n",
        "    }\n",
        "\n",
        "# Function to extract features with a given window size\n",
        "def sent2features_window(sent, window):\n",
        "    return [word2features(sent, i - window) if i >= window else word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "# Function to train and test a CRF model with given features\n",
        "def train_test_crf_model(features_train, labels_train, features_test):\n",
        "    crf_model = CRF(\n",
        "        algorithm='lbfgs',\n",
        "        c1=0.1,\n",
        "        c2=0.1,\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True\n",
        "    )\n",
        "    crf_model.fit(features_train, labels_train)\n",
        "    pred_labels = crf_model.predict(features_test)\n",
        "    return pred_labels\n",
        "\n",
        "\n",
        "# Add the \"suffix\" feature\n",
        "def sent2spacy_suffix_features(sent):\n",
        "    nlp = es_core_news_sm.load()\n",
        "    nlp.tokenizer = Tokenizer(nlp.vocab)\n",
        "    spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
        "    feats = []\n",
        "    for i, token in enumerate(spacy_sent):\n",
        "        token_feats = {\n",
        "            'bias': 1.0,\n",
        "            'word.lower()': token.lower_,\n",
        "            'pos': token.pos_,\n",
        "            'lemma': token.lemma_,\n",
        "            'suffix': token.text[-3:]\n",
        "        }\n",
        "        feats.append(token_feats)\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the conll2002 dataset\n",
        "nltk.download('conll2002')\n",
        "trn_sents = conll2002.iob_sents('esp.train')\n",
        "tst_sents = conll2002.iob_sents('esp.testa')\n",
        "\n",
        "trn_sents = trn_sents[:10]\n",
        "tst_sents = tst_sents[:10]\n",
        "\n",
        "# Baseline using sent2spacy_features\n",
        "trn_feats_baseline = [sent2spacy_features(s) for s in trn_sents]\n",
        "tst_feats_baseline = [sent2spacy_features(s) for s in tst_sents]\n",
        "trn_labels = [sent2labels(s) for s in trn_sents]\n",
        "\n",
        "pred_labels_baseline = train_test_crf_model(trn_feats_baseline, trn_labels, tst_feats_baseline)\n",
        "hyp_baseline = [[(tst_feats_baseline[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred_labels_baseline)]\n",
        "results_baseline = evaluate(tst_sents, hyp_baseline)\n",
        "pd_tbl_baseline = pd.DataFrame().from_dict(results_baseline, orient='index')\n",
        "pd_tbl_baseline.round(decimals=3)\n",
        "\n",
        "# Add the \"suffix\" feature\n",
        "# def sent2spacy_suffix_features(sent):\n",
        "#     nlp = es_core_news_sm.load()\n",
        "#     nlp.tokenizer = Tokenizer(nlp.vocab)\n",
        "#     spacy_sent = nlp(\" \".join(sent2tokens(sent)))\n",
        "#     feats = []\n",
        "#     for i, token in enumerate(spacy_sent):\n",
        "#         token_feats = {\n",
        "#             'bias': 1.0,\n",
        "#             'word.lower()': token.lower_,\n",
        "#             'pos': token.pos_,\n",
        "#             'lemma': token.lemma_,\n",
        "#             'suffix': token.text[-3:]\n",
        "#         }\n",
        "#         feats.append(token_feats)\n",
        "#     return feats\n",
        "\n",
        "trn_feats_suffix = [sent2spacy_suffix_features(s) for s in trn_sents]\n",
        "tst_feats_suffix = [sent2spacy_suffix_features(s) for s in tst_sents]\n",
        "\n",
        "pred_labels_suffix = train_test_crf_model(trn_feats_suffix, trn_labels, tst_feats_suffix)\n",
        "hyp_suffix = [[(tst_feats_suffix[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred_labels_suffix)]\n",
        "results_suffix = evaluate(tst_sents, hyp_suffix)\n",
        "pd_tbl_suffix = pd.DataFrame().from_dict(results_suffix, orient='index')\n",
        "pd_tbl_suffix.round(decimals=3)\n",
        "\n",
        "# Add all features used in the tutorial\n",
        "trn_feats_tutorial = [sent2features_tutorial(s) for s in trn_sents]\n",
        "tst_feats_tutorial = [sent2features_tutorial(s) for s in tst_sents]\n",
        "\n",
        "pred_labels_tutorial = train_test_crf_model(trn_feats_tutorial, trn_labels, tst_feats_tutorial)\n",
        "hyp_tutorial = [[(tst_feats_tutorial[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred_labels_tutorial)]\n",
        "results_tutorial = evaluate(tst_sents, hyp_tutorial)\n",
        "pd_tbl_tutorial = pd.DataFrame().from_dict(results_tutorial, orient='index')\n",
        "pd_tbl_tutorial.round(decimals=3)\n",
        "\n",
        "# Increase feature window to [-1, +1]\n",
        "trn_feats_window_1 = [sent2features_window(s, 1) for s in trn_sents]\n",
        "tst_feats_window_1 = [sent2features_window(s, 1) for s in tst_sents]\n",
        "\n",
        "pred_labels_window_1 = train_test_crf_model(trn_feats_window_1, trn_labels, tst_feats_window_1)\n",
        "hyp_window_1 = [[(tst_feats_window_1[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred_labels_window_1)]\n",
        "results_window_1 = evaluate(tst_sents, hyp_window_1)\n",
        "pd_tbl_window_1 = pd.DataFrame().from_dict(results_window_1, orient='index')\n",
        "pd_tbl_window_1.round(decimals=3)\n",
        "\n",
        "# Increase feature window to [-2, +2]\n",
        "trn_feats_window_2 = [sent2features_window(s, 2) for s in trn_sents]\n",
        "tst_feats_window_2 = [sent2features_window(s, 2) for s in tst_sents]\n",
        "\n",
        "pred_labels_window_2 = train_test_crf_model(trn_feats_window_2, trn_labels, tst_feats_window_2)\n",
        "hyp_window_2 = [[(tst_feats_window_2[i][j], t) for j, t in enumerate(tokens)] for i, tokens in enumerate(pred_labels_window_2)]\n",
        "results_window_2 = evaluate(tst_sents, hyp_window_2)\n",
        "pd_tbl_window_2 = pd.DataFrame().from_dict(results_window_2, orient='index')\n",
        "pd_tbl_window_2.round(decimals=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "Q2hRatm1hCiE",
        "outputId": "b332ea8e-9de5-4669-f161-8a6e8c60e1f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2002 is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:887: UserWarning: [W095] Model 'es_core_news_sm' (3.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.5.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  ) from None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-496a7de0f6b1>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrn_feats_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2spacy_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrn_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtst_feats_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2spacy_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtst_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrn_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpred_labels_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_crf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_feats_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_feats_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-496a7de0f6b1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrn_feats_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2spacy_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrn_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtst_feats_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2spacy_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtst_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrn_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpred_labels_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_crf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_feats_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_feats_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3eb5d898c9f3>\u001b[0m in \u001b[0;36msent2labels\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent2labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;31m# Function to extract tokens using sent2tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3eb5d898c9f3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent2labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;31m# Function to extract tokens using sent2tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}